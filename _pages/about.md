---
permalink: /
title: "About Me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a Master's student in Electronic Engineering at Columbia University, focusing on robotics, computer vision, and reinforcement learning. I have had the privilege of being advised by Professor Matei Ciocarlie at the Robotic Manipulation and Mobility [(ROAM)](https://roam.me.columbia.edu/) Lab and Professor Lerrel Pinto at the General-purpose Robotics and AI Lab [(GRAIL)](https://www.lerrelpinto.com/), where I serve as a research assistant. Additionally, I'm a member of the [Creative Machines Lab](https://www.creativemachineslab.com/), guided by Professor Hod Lipson, contributing to advancements at the intersection of robotics and AI.

Before coming to Columbia University, I worked as a research intern in Professor [Dongbin Zhao's](https://people.ucas.ac.cn/~zhaodongbin?language=en) lab at the Institute of Automation, UCAS, where I focused on embedded systems and reinforcement learning.


[Email](yc4317@columbia.edu) / [LinkedIn](https://www.linkedin.com/in/yifengcao/)




## Publications

### Learning Precise, Contact-Rich Manipulation through Uncalibrated Tactile Skins

ViSk builds on BAKU, a SOTA policy architecture, and AnySkin â€“ a magnetic tactile sensor. We present comprehensive evaluations on four tasks requiring mm-scale precision: Plug insertion, USB insertion, Card swiping, and Book retrieval, and see an average improvement of ~27.5% when using ViSk over vision-only policies across the four tasks. 

Additionally, the most exciting part of ViSk is the extent of generalizability of learned policies; it can also perform really well in both unseen spatial configurations of the environment as well as unseen variants of the grasped objects.

<img src="images/visk_git.jpg" alt="Visk Demo" width="350">

<small>**Published:** Oct 22, 2024 </small> 
<small>**Venue:** arXiv </small> 

**[Download Paper](https://arxiv.org/pdf/2410.17246)**  
**[Project Website](https://visuoskin.github.io/)**

### AnySkin: Plug-and-play Skin Sensing for Robotic Touch

While tactile sensing is widely accepted as an important and useful sensing modality, its use pales in comparison to other sensory modalities like vision and proprioception. AnySkin addresses the critical challenges that impede the use of tactile sensing -- versatility, replaceability, and data reusability.

<img src="images/anyskin_git.jpg" alt="AnySkin Demo" width="350">

<small>**Published:** Sep 27, 2024 </small> 
<small>**Venue:** arXiv </small> 

**[Download Paper](https://arxiv.org/pdf/2409.08276)**  
**[Project Website](https://anon-visk.github.io/)**


### An auxiliary tool for preliminary tests of skin cancer: A self-modifying meta-learning method for clean and noisy data

In this paper, we propose a self-modifying meta-learning model which combines the idea of meta-learning with curriculum learning. Applying this mechanism, our model will first train on data of common diseases and then adapt the model to rare disease classification.

**[Download Paper](https://ieeexplore.ieee.org/abstract/document/9696123)**  

---
